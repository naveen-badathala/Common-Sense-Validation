{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flask.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVYax21P4bzK",
        "outputId": "072c3e1e-2f22-4d88-e607-4129cb223b4c"
      },
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install transformers -q\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.7/dist-packages (0.0.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.12.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TF_hCnfuowFc",
        "outputId": "49555d68-d2c7-469e-9621-b792129b36f6"
      },
      "source": [
        "\"\"\"\n",
        "Uncomment the code below if you want to load the files from drive\n",
        "\"\"\"\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nUncomment the code below if you want to load the files from drive\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb_mr5iLszEl"
      },
      "source": [
        "from pickle import load\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKGxkpwTle0U"
      },
      "source": [
        "if torch.cuda.is_available():      \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNuv9Wu2lLC3"
      },
      "source": [
        "# model_names = ['bert-large-cased','albert-xxlarge-v2','roberta-large','xlnet-large-cased']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WZsHJxUXMNsV",
        "outputId": "0eb8976d-9787-46c9-d8b2-7b75217e0573"
      },
      "source": [
        "\"\"\"\n",
        "Change the path in the following cells to their appropriate trained models shared.\n",
        "\"\"\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nChange the path in the following cells to their appropriate trained models shared.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C5ehD13pRD3"
      },
      "source": [
        "single_models_directory = {\"bert\":\"/content/drive/MyDrive/IIT_Bombay/CS 626/Single_Models/BERT\",\n",
        "                    \"roberta\":\"/content/drive/MyDrive/IIT_Bombay/CS 626/Single_Models/RoBERTa\",\n",
        "                    \"xlnet\":\"/content/drive/MyDrive/IIT_Bombay/CS 626/Single_Models/XLNet\",\n",
        "                    \"albert\":\"/content/drive/MyDrive/IIT_Bombay/CS 626/Single_Models/ALBERT\"}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbF7ppcqmYC0"
      },
      "source": [
        "two_sentence_models_directory = {\"bert\":\"/content/drive/MyDrive/IIT_Bombay/CS 626/Two_Sentences_Models/BERT\",\n",
        "                    \"roberta\":\"/content/drive/MyDrive/IIT_Bombay/CS 626/Two_Sentences_Models/Roberta\",\n",
        "                    \"xlnet\":\"/content/drive/MyDrive/IIT_Bombay/CS 626/Two_Sentences_Models/XL-Net\",\n",
        "                    \"albert\":\"/content/drive/MyDrive/IIT_Bombay/CS 626/Two_Sentences_Models/ALBERT\"}"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I-I-UrROz_t"
      },
      "source": [
        "sbert_sent_transformer = '/content/drive/MyDrive/IIT_Bombay/CS 626/trained_model_with_reasons_epoch10'\n",
        "sbert_single_sentence = '/content/drive/MyDrive/IIT_Bombay/CS 626/svm_models/trained_svm.sav'\n",
        "sbert_two_sentences = '/content/drive/MyDrive/IIT_Bombay/CS 626/svm_models/trained_svm_on_pairs.sav'"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGAlYuy0o37u"
      },
      "source": [
        "def load_model(m,c):\n",
        "  if c==\"s\":\n",
        "    models_directory = single_models_directory\n",
        "  else:\n",
        "    models_directory = two_sentence_models_directory\n",
        "  import torch\n",
        "  from transformers import AutoModelForSequenceClassification,AutoTokenizer,AdamW\n",
        "\n",
        "  # Load a trained model and vocabulary that you have fine-tuned\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(models_directory[m])\n",
        "  tokenizer = AutoTokenizer.from_pretrained(models_directory[m])\n",
        "\n",
        "  # Copy the model to the GPU.\n",
        "  model.to(device)\n",
        "  return model, tokenizer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vBgLDfOn3gL"
      },
      "source": [
        "def predict_for_a_sentence(sentence0,model,tokenizer):\n",
        "  import numpy as np\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "  from scipy.special import softmax\n",
        "\n",
        "  # sentences = np.array([sentence0])\n",
        "  labels = np.array([0])\n",
        "\n",
        "  input_ids = []\n",
        "\n",
        "  encoded_sent = tokenizer.encode(\n",
        "                        sentence0,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "  input_ids.append(encoded_sent)\n",
        "  input_ids = pad_sequences(input_ids, maxlen=32, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # Create a mask of 1s for each token followed by 0s for padding\n",
        "  for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "  # Convert to tensors.\n",
        "  prediction_inputs = torch.tensor(input_ids)\n",
        "  prediction_masks = torch.tensor(attention_masks)\n",
        "  prediction_labels = torch.tensor(labels)\n",
        "\n",
        "  # Set the batch size.  \n",
        "  batch_size = 32  \n",
        "\n",
        "  # Create the DataLoader.\n",
        "  prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "  prediction_sampler = SequentialSampler(prediction_data)\n",
        "  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  return np.argmax(predictions[0], axis=1).flatten(),np.max(softmax(np.array(logits))[0])\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJa6anLbmzcD"
      },
      "source": [
        "def predict_for_two_sentences(sentence0,sentence1,model,tokenizer):\n",
        "  from scipy.special import softmax\n",
        "  import numpy as np\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "  MAX_LEN = 64\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  labels = np.array([0])\n",
        "  input_ids = []\n",
        "  input_type_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  tokenizer.padding_side = 'right'\n",
        "  encoded_sent = tokenizer.encode_plus(\n",
        "                          sentence0,            # Sentence to encode.\n",
        "                          sentence1,\n",
        "                          add_special_tokens = True, \n",
        "                          max_length = MAX_LEN,\n",
        "                          pad_to_max_length = True\n",
        "                          )\n",
        "\n",
        "  #Add the encoded sentence to the list.\n",
        "  input_ids.append(encoded_sent['input_ids'])\n",
        "  # input_type_ids.append(encoded_sent['token_type_ids'])\n",
        "  attention_masks.append(encoded_sent['attention_mask'])\n",
        "\n",
        "  # Convert to tensors.\n",
        "  prediction_inputs = torch.tensor(input_ids)\n",
        "  prediction_masks = torch.tensor(attention_masks)\n",
        "  prediction_labels = torch.tensor(labels)\n",
        "\n",
        "  # Set the batch size.  \n",
        "  batch_size = 32  \n",
        "\n",
        "  # Create the DataLoader.\n",
        "  prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "  prediction_sampler = SequentialSampler(prediction_data)\n",
        "  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "    return np.argmax(predictions[0], axis=1).flatten(), np.max(softmax(np.array(logits))[0])\n",
        "    # return predictions"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgiXgezqqKSt"
      },
      "source": [
        "bert_model,bert_tokenizer = load_model(\"bert\",\"s\")\n",
        "albert_model,albert_tokenizer = load_model(\"albert\",\"s\")\n",
        "roberta_model,roberta_tokenizer = load_model(\"roberta\",\"s\")\n",
        "xlnet_model,xlnet_tokenizer = load_model(\"xlnet\",\"s\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTS93yBHl5KX"
      },
      "source": [
        "bert_model_2,bert_tokenizer_2 = load_model(\"bert\",\"t\")\n",
        "albert_model_2,albert_tokenizer_2 = load_model(\"albert\",\"t\")\n",
        "roberta_model_2,roberta_tokenizer_2 = load_model(\"roberta\",\"t\")\n",
        "xlnet_model_2,xlnet_tokenizer_2 = load_model(\"xlnet\",\"t\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1h1rTvyvPHf"
      },
      "source": [
        "def predict_single_sent_sbert(input_sentence):\n",
        "    sbert_model = SentenceTransformer(sbert_sent_transformer)\n",
        "    svm_model = load(open(sbert_single_sentence, 'rb'))\n",
        "    sentence = [input_sentence]\n",
        "    sentence_embedding = sbert_model.encode(sentence)\n",
        "    prediction = svm_model.predict(sentence_embedding)\n",
        "    pred_prob = svm_model.predict_proba(sentence_embedding)\n",
        "    print(prediction)\n",
        "    print(pred_prob)\n",
        "    return prediction[0], pred_prob[0][1]\n",
        "    \n",
        "\n",
        "def predict_pairs_sbert(input1, input2):\n",
        "    sbert_model = SentenceTransformer(sbert_sent_transformer)\n",
        "    svm_model = load(open(sbert_two_sentences, 'rb'))\n",
        "    sentence = [input1, input2]\n",
        "    sentence_embedding = sbert_model.encode(sentence)\n",
        "    input_embedding = [np.concatenate((sentence_embedding[0], sentence_embedding[1]))]\n",
        "    prediction = svm_model.predict(input_embedding)\n",
        "    pred_prob = svm_model.predict_proba(input_embedding)\n",
        "    print(prediction)\n",
        "    print(pred_prob)\n",
        "    return prediction[0], pred_prob[0][prediction[0]]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EIQmw_Gvo0H",
        "outputId": "0278ad35-a7bd-47ba-b8d1-f40db13d02ba"
      },
      "source": [
        "label, score = predict_single_sent_sbert('An apple is in the moon')\n",
        "print(\"Label: \", label, \" Score: \", score)\n",
        "label, score = predict_pairs_sbert('An apple is in the refrigerator', 'An elephant is in the refrigerator')\n",
        "print(\"Label: \", label, \" Score: \", score)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n",
            "[[0.97882327 0.02117673]]\n",
            "Label:  0  Score:  0.021176733891169686\n",
            "[0]\n",
            "[[0.99751757 0.00248243]]\n",
            "Label:  0  Score:  0.9975175661909539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPYsWew9qVYa",
        "outputId": "2a80530b-822b-4179-bd53-63e628161914"
      },
      "source": [
        "predict_for_a_sentence(\"he eats spoon\",bert_model_2,bert_tokenizer_2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1]), 0.85771805)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RBqRFKMpLb4",
        "outputId": "e1324105-6949-4a13-b33d-a4f724036b34"
      },
      "source": [
        "predict_for_two_sentences(\"he drinks milk\",\"he drinks spoon\",bert_model_2,bert_tokenizer_2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1]), 0.757974)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbaX0lxZ4xVh",
        "outputId": "5d04b4d1-9602-4ca2-da47-bfac24addfc3"
      },
      "source": [
        "\"\"\"\n",
        "Add the templates folder containing the index.html to the folder. \n",
        "Change the form action url in html to the ngrok url after running this cell.\n",
        "\"\"\"\n",
        "\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "from flask_ngrok import run_with_ngrok\n",
        "import json\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def hello_world():\n",
        "    \"\"\"\n",
        "    Demo function to check the api calls.\n",
        "\n",
        "    Returns : string\n",
        "    \"\"\"\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route('/getprediction',methods=['POST'])\n",
        "def getprediction():   \n",
        "  inputs = request.form.values()\n",
        "  sentences = []\n",
        "  for i in inputs:\n",
        "    sentences.append(i)\n",
        "  \n",
        "  if len(sentences[1])==0:\n",
        "    bert_output = predict_for_a_sentence(sentences[0],bert_model,bert_tokenizer)\n",
        "    albert_output = predict_for_a_sentence(sentences[0],albert_model,albert_tokenizer)\n",
        "    roberta_output = predict_for_a_sentence(sentences[0],roberta_model,roberta_tokenizer)\n",
        "    xlnet_output = predict_for_a_sentence(sentences[0],xlnet_model,xlnet_tokenizer)\n",
        "    sbert_label, sbert_score = predict_single_sent_sbert(sentences[0])\n",
        "\n",
        "    input_type = \"Single Sentence\"\n",
        "    input_text = sentences[0]\n",
        "    if bert_output[0][0]==1:\n",
        "      bert_sense = \"no sense\"\n",
        "      bert_score = str(1 - bert_output[1])\n",
        "    else:\n",
        "      bert_sense = \"makes sense\"\n",
        "      bert_score = str(bert_output[1])\n",
        "    if albert_output[0][0]==1:\n",
        "      albert_sense = \"no sense\"\n",
        "      albert_score = str(1-albert_output[1])\n",
        "    else:\n",
        "      albert_sense = \"makes sense\"\n",
        "      albert_score = str(albert_output[1])\n",
        "    if roberta_output[0][0]==1:\n",
        "      roberta_sense = \"no sense\"\n",
        "      roberta_score = str(1-roberta_output[1])\n",
        "    else:\n",
        "      roberta_sense = \"makes sense\"\n",
        "      roberta_score = str(roberta_output[1])\n",
        "    if xlnet_output[0][0]==1:\n",
        "      xlnet_sense = \"no sense\"\n",
        "      xlnet_score = str(1-xlnet_output[1])\n",
        "    else:\n",
        "      xlnet_sense = \"makes sense\"\n",
        "      xlnet_score = str(xlnet_output[1])\n",
        "    sbert_sense = \"no sense\" if (sbert_label == 0) else \"makes sense\"\n",
        "    sbert_score = str(sbert_score)\n",
        "    \n",
        "\n",
        "        \n",
        "        \n",
        "    output_sentence = sentences[0]\n",
        "    # bert = \"BERT :    \" + bert_sense + \"   ---> \" + str(bert_output[1])\n",
        "    # albert = \"ALBERT :  \" + albert_sense + \"   ---> \" + str(albert_output[1])\n",
        "    # roberta = \"ROBERTA : \" + roberta_sense + \"   ---> \" + str(roberta_output[1])\n",
        "    # xlnet = \"XLNET :   \" + xlnet_sense + \"   ---> \" + str(xlnet_output[1])\n",
        "    # bert_score = str(bert_output[1])\n",
        "    # albert_score = str(albert_output[1])\n",
        "    # roberta_score = str(roberta_output[1])\n",
        "    # xlnet_score = str(xlnet_output[1])\n",
        "    # sbert_score = str(sbert_score)\n",
        "  else:\n",
        "    bert_output, bert_score = predict_for_two_sentences(sentences[0],sentences[1],bert_model_2,bert_tokenizer_2)\n",
        "    albert_output, albert_score = predict_for_two_sentences(sentences[0],sentences[1],albert_model_2,albert_tokenizer_2)\n",
        "    roberta_output, roberta_score = predict_for_two_sentences(sentences[0],sentences[1],roberta_model_2,roberta_tokenizer_2)\n",
        "    xlnet_output, xlnet_score = predict_for_two_sentences(sentences[0],sentences[1],xlnet_model_2,xlnet_tokenizer_2)\n",
        "    sbert_label, sbert_score = predict_pairs_sbert(sentences[0], sentences[1])\n",
        "\n",
        "    input_type = \"Sentence Pair\"\n",
        "    input_text = sentences[0] + \" , \" + sentences[1]\n",
        "\n",
        "    output_header = \"Sentence that makes sense: \"\n",
        "    if bert_output[0]==1:\n",
        "      bert_sense = output_header + sentences[0]\n",
        "      # bert_sense = sentences[0] + \" : makes sense, \" + sentences[1] + \" : no sense\" \n",
        "    else:\n",
        "      # bert_sense = sentences[0] + \" : no sense,    \" + sentences[1] + \" : makes sense\" \n",
        "      bert_sense = output_header + sentences[1]\n",
        "    if albert_output[0]==1:\n",
        "      # albert_sense = sentences[0] + \" : makes sense, \" + sentences[1] + \" : no sense\" \n",
        "      albert_sense = output_header + sentences[0]\n",
        "    else:\n",
        "      # albert_sense = sentences[0] + \" : no sense,    \" + sentences[1] + \" : makes sense\" \n",
        "      albert_sense = output_header + sentences[1]\n",
        "    if roberta_output[0]==1:\n",
        "      # roberta_sense = sentences[0] + \" : makes sense, \" + sentences[1] + \" : no sense\" \n",
        "      roberta_sense = output_header + sentences[0]\n",
        "    else:\n",
        "      # roberta_sense = sentences[0] + \" : no sense,    \" + sentences[1] + \" : makes sense\" \n",
        "      roberta_sense = output_header + sentences[1]\n",
        "    if xlnet_output[0]==1:\n",
        "      # xlnet_sense = sentences[0] + \" : makes sense, \" + sentences[1] + \" : no sense\" \n",
        "      xlnet_sense = output_header + sentences[0]\n",
        "    else:\n",
        "      # xlnet_sense = sentences[0] + \" : no sense,    \" + sentences[1] + \" : makes sense\" \n",
        "      xlnet_sense = output_header + sentences[1]\n",
        "\n",
        "\n",
        "    bert_score = str(bert_score)\n",
        "    albert_score = str(albert_score)\n",
        "    roberta_score = str(roberta_score)\n",
        "    xlnet_score = str(xlnet_score)\n",
        "    sbert_score = str(sbert_score)\n",
        "    \n",
        "    sbert_sense = \"Sentence that makes sense: \"\n",
        "    sbert_sense += sentences[0] if (sbert_label == 0) else sentences[1]\n",
        "        \n",
        "  # return render_template(\"index.html\", bert=\"BERT    ----> \"+ bert_sense, albert=\"ALBERT  ----> \"+albert_sense, roberta=\"ROBERTA t----> \"+roberta_sense, xlnet=\"XLNET   ----> \"+xlnet_sense, sbert=\"SBERT   ---> \"+ sbert_sense)\n",
        "  return render_template(\"index.html\", bert=bert_sense, albert=albert_sense, roberta=roberta_sense, xlnet=xlnet_sense, sbert=sbert_sense, bert_sc = bert_score, albert_sc = albert_score, roberta_sc = roberta_score, xlnet_sc = xlnet_score, sbert_sc = sbert_score, selection = input_type, input_txt = input_text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://bfc9-34-80-255-190.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [28/Nov/2021 06:39:27] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [28/Nov/2021 06:39:29] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [28/Nov/2021 06:39:29] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [28/Nov/2021 06:39:34] \"\u001b[31m\u001b[1mGET /getprediction HTTP/1.1\u001b[0m\" 405 -\n",
            "127.0.0.1 - - [28/Nov/2021 06:39:34] \"\u001b[37mPOST /getprediction HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n",
            "[[0.0128885 0.9871115]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "127.0.0.1 - - [28/Nov/2021 06:39:48] \"\u001b[37mPOST /getprediction HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n",
            "[[0.99657874 0.00342126]]\n"
          ]
        }
      ]
    }
  ]
}